Tensorflow Code with steps and comments
a.	Import all the variables
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

b.	reads a CSV file named MSFT.csv
msft = pd.read_csv('msft.csv',index_col='Date')


c.	Check if the dataset is ready to proceed further
msft.head()
msft.index = pd.to_datetime(msft.index)
d.	Split it into train and test data
train_set = pd.DataFrame(msft.head(297)['Open'])
test_set = pd.DataFrame(msft.tail(12)['Open'])

e.	Import minmax scaler and create a scaler object
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

f.	Train scaled data and fit and transform train data
train_scaled = scaler.fit_transform(train_set)
test_scaled = scaler.transform(test_set)

g.	Define a batch function
def next_batch(training_data,batch_size,steps): 

# Grab a random starting point for each batch
 rand_start = np.random.randint(0,len(training_data)-steps) 
 
# Create Y data for time series in the batches
y_batch=np.array(training_data[rand_start:rand_start+steps+1]).reshape(1,steps+1)
return y_batch[:, :-1].reshape(-1, steps, 1), y_batch[:, 1:].reshape(-1, steps,1) 
rand_start = np.random.randint(0,10) 
steps=3
rand_start
y_batch = np.array(train_scaled[rand_start:rand_start+steps+1]).reshape(1,steps+1)
print(rand_start,y_batch,y_batch[:, :-1].reshape(-1, steps, 1),y_batch[:, 1:].reshape(-1, steps, 1))

h.	Setting up the RNN Model
import tensorflow as tf
tf.reset_default_graph()

i.	constants of the model
num_inputs = 1
num_time_steps = 12
num_neurons = 100
num_outputs = 1
learning_rate = 0.001 
num_train_iterations = 4000
batch_size = 1

j.	place holders
X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])
y = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs])

k.	Cell
cell = tf.contrib.rnn.OutputProjectionWrapper(tf.con	trib.rnn.BasicLSTMCell(num_units=num_neurons, activation=tf.nn.relu),output_size=num_outputs)

outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)

l.	loss function and optimizer
loss = tf.reduce_mean(tf.square(outputs - y)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
train = optimizer.minimize(loss)
m.	initialize the global variable
init = tf.global_variables_initializer()
n.	using the tf saver option
saver = tf.train.Saver()
o.	running the session
with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
sess.run(init)
for iteration in range(num_train_iterations):    
X_batch, y_batch = next_batch(train_scaled,batch_size,num_time_steps)
    	sess.run(train, feed_dict={X: X_batch, y: y_batch})
 		if iteration % 100 == 0:
mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
print(iteration, "\tMSE:", mse)

p.	Save the model
saver.save(sess, "./testmodel")
q.	Predicting the future
i.	test_set
r.	running the prediction session
with tf.Session() as sess:

 # Use your Saver instance to restore your saved rnn time series model
 saver.restore(sess, "./ testmodel ")

 train_set1 = pd.DataFrame(milk.head(310)['Open'])
train_scaled1 = scaler.fit_transform(train_set1)
 w1=24
 w2=12
 train=[]
train_seed1 = list(train_scaled[-w1:])
 train = list(train)
for i in range(12):
 X_batch = np.array(train_seed1[-w1:-w2]).reshape(1, num_time_steps, 1)
w1=w1-1
 w2=w2-1
y_pred = sess.run(outputs, feed_dict={X: X_batch})
train.append(y_pred[0, -1, 0])
print(y_pred[0, -1, 0])
results = scaler.inverse_transform(np.array(train).reshape(12,1))
test_set['Generated'] = scaler.inverse_transform(np.array(train).reshape(12,1))
test_set.drop(['Generated1'],axis=1,inplace=True)
df = test_set.reset_index(inplace=True)
test_set.plot()
i.	 
